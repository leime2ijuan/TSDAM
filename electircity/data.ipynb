{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c7d1547-7ef4-4b82-9ac6-bd6c5e161779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最常见的时间间隔: 0 days 01:00:00\n",
      "数据时间范围: 2016-01-01 00:00:00 到 2017-12-31 23:00:00\n",
      "将处理以下列: ['Hog_lodging_Brian', 'Hog_lodging_Nikki', 'Hog_lodging_Ora', 'Robin_lodging_Celia', 'Robin_lodging_Elmer', 'Robin_lodging_Renea', 'Hog_health_Hisako', 'Hog_health_Jenny', 'Hog_health_Kesha', 'Rat_health_Gaye', 'Rat_health_Guy', 'Rat_health_Shane', 'Rat_public_Chrissy', 'Eagle_public_Pearle', 'Hog_public_Crystal', 'Hog_public_Kevin', 'Hog_public_Octavia', 'Rat_public_Roma', 'Eagle_office_Ryan', 'Rat_office_Tracy', 'Robin_office_Lindsay', 'Wolf_office_Bobbie', 'Hog_office_Sung', 'Wolf_office_Cary', 'Hog_education_Hallie', 'Hog_education_Haywood', 'Hog_education_Janell', 'Hog_education_Rachael', 'Hog_education_Wayne', 'Robin_education_Zenia']\n",
      "检测到0个异常值在'Hog_lodging_Brian'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Hog_lodging_Nikki'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Hog_lodging_Ora'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Robin_lodging_Celia'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Robin_lodging_Elmer'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Robin_lodging_Renea'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Hog_health_Hisako'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Hog_health_Jenny'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Hog_health_Kesha'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Rat_health_Gaye'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Rat_health_Guy'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Rat_health_Shane'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Rat_public_Chrissy'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Eagle_public_Pearle'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Hog_public_Crystal'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Hog_public_Kevin'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Hog_public_Octavia'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Rat_public_Roma'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Eagle_office_Ryan'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Rat_office_Tracy'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Robin_office_Lindsay'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Wolf_office_Bobbie'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Hog_office_Sung'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Wolf_office_Cary'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Hog_education_Hallie'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Hog_education_Haywood'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Hog_education_Janell'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Hog_education_Rachael'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Hog_education_Wayne'列 (修改后的IQR方法)\n",
      "检测到0个异常值在'Robin_education_Zenia'列 (修改后的IQR方法)\n",
      "'Hog_lodging_Nikki'列中有107个缺失值\n",
      "'Hog_lodging_Ora'列中有1个缺失值\n",
      "'Robin_lodging_Celia'列中有49个缺失值\n",
      "'Robin_lodging_Elmer'列中有117个缺失值\n",
      "'Robin_lodging_Renea'列中有102个缺失值\n",
      "'Hog_health_Hisako'列中有70个缺失值\n",
      "'Hog_health_Jenny'列中有23个缺失值\n",
      "'Hog_health_Kesha'列中有25个缺失值\n",
      "'Rat_health_Gaye'列中有257个缺失值\n",
      "'Rat_health_Guy'列中有170个缺失值\n",
      "'Rat_health_Shane'列中有227个缺失值\n",
      "'Rat_public_Chrissy'列中有405个缺失值\n",
      "'Eagle_public_Pearle'列中有350个缺失值\n",
      "'Hog_public_Crystal'列中有69个缺失值\n",
      "'Hog_public_Kevin'列中有350个缺失值\n",
      "'Hog_public_Octavia'列中有165个缺失值\n",
      "'Rat_public_Roma'列中有52个缺失值\n",
      "'Eagle_office_Ryan'列中有215个缺失值\n",
      "'Rat_office_Tracy'列中有52个缺失值\n",
      "'Wolf_office_Bobbie'列中有351个缺失值\n",
      "'Hog_office_Sung'列中有589个缺失值\n",
      "'Wolf_office_Cary'列中有18个缺失值\n",
      "'Hog_education_Hallie'列中有58个缺失值\n",
      "'Hog_education_Haywood'列中有184个缺失值\n",
      "'Hog_education_Janell'列中有365个缺失值\n",
      "'Hog_education_Wayne'列中有15个缺失值\n",
      "数据处理完成!\n",
      "验证通过: 所有列中的非零值都被正确保留\n",
      "\n",
      "数据处理前后统计比较:\n",
      "                       原始数据缺失值  处理后缺失值  原始数据零值数量  处理后零值数量       原始数据均值  \\\n",
      "Hog_lodging_Brian            0       0         1        1   105.500399   \n",
      "Hog_lodging_Nikki          107       0         0        0    58.441681   \n",
      "Hog_lodging_Ora              1       0         0        0   220.748467   \n",
      "Robin_lodging_Celia         49       0         0        0    51.381862   \n",
      "Robin_lodging_Elmer        117       0         0        0    94.303147   \n",
      "Robin_lodging_Renea        102       0         0        0    90.396560   \n",
      "Hog_health_Hisako           70       0         0        0   206.453696   \n",
      "Hog_health_Jenny            23       0         0        0   181.719111   \n",
      "Hog_health_Kesha            25       0         0        0   246.194194   \n",
      "Rat_health_Gaye            257       0         0        0    37.472391   \n",
      "Rat_health_Guy             170       0         8        8  1464.699873   \n",
      "Rat_health_Shane           227       0         0        0  1146.168417   \n",
      "Rat_public_Chrissy         405       0         4        4    55.731305   \n",
      "Eagle_public_Pearle        350       0       422      427   346.956293   \n",
      "Hog_public_Crystal          69       0         0        0   298.975474   \n",
      "Hog_public_Kevin           350       0         0        0   523.694040   \n",
      "Hog_public_Octavia         165       0         0        0   195.293854   \n",
      "Rat_public_Roma             52       0         2        2    53.289017   \n",
      "Eagle_office_Ryan          215       0        52       52    26.275377   \n",
      "Rat_office_Tracy            52       0        11       11    20.585011   \n",
      "Robin_office_Lindsay         0       0         0        0    45.640760   \n",
      "Wolf_office_Bobbie         351       0         1        1    27.249742   \n",
      "Hog_office_Sung            589       0         0        0    46.614894   \n",
      "Wolf_office_Cary            18       0         0        0   244.769522   \n",
      "Hog_education_Hallie        58       0         0        0    65.458110   \n",
      "Hog_education_Haywood      184       0         0        0   139.044577   \n",
      "Hog_education_Janell       365       0         1        1  3331.398673   \n",
      "Hog_education_Rachael        0       0         0        0    74.879867   \n",
      "Hog_education_Wayne         15       0         0        0   165.054249   \n",
      "Robin_education_Zenia        0       0         0        0   200.968600   \n",
      "\n",
      "                             处理后均值    原始数据中位数      处理后中位数  \n",
      "Hog_lodging_Brian       105.500399    98.0000    98.00000  \n",
      "Hog_lodging_Nikki        58.619562    54.9720    55.14000  \n",
      "Hog_lodging_Ora         220.748416   225.0000   225.00000  \n",
      "Robin_lodging_Celia      51.406334    48.2000    48.20000  \n",
      "Robin_lodging_Elmer      94.347049    92.4000    92.40000  \n",
      "Robin_lodging_Renea      90.458559    88.6000    88.60000  \n",
      "Hog_health_Hisako       206.520879   197.2095   197.27800  \n",
      "Hog_health_Jenny        181.716164   168.3120   168.31100  \n",
      "Hog_health_Kesha        246.175721   235.2220   235.21200  \n",
      "Rat_health_Gaye          37.757055    35.0400    35.25000  \n",
      "Rat_health_Guy         1464.952421  1408.0800  1410.96000  \n",
      "Rat_health_Shane       1146.485684  1145.3400  1145.34000  \n",
      "Rat_public_Chrissy       56.075347    52.3100    52.76000  \n",
      "Eagle_public_Pearle     349.325391   398.3235   401.10250  \n",
      "Hog_public_Crystal      299.589079   296.3130   297.42950  \n",
      "Hog_public_Kevin        534.456462   458.8305   466.16700  \n",
      "Hog_public_Octavia      195.694316   192.2960   192.77500  \n",
      "Rat_public_Roma          53.280863    51.6700    51.67000  \n",
      "Eagle_office_Ryan        26.333932    24.0000    24.00000  \n",
      "Rat_office_Tracy         20.597492    20.0600    20.09000  \n",
      "Robin_office_Lindsay     45.640760    51.7500    51.75000  \n",
      "Wolf_office_Bobbie       27.596697    20.8425    20.97500  \n",
      "Hog_office_Sung          47.195620    42.7970    43.21250  \n",
      "Wolf_office_Cary        244.799871   209.0325   209.05750  \n",
      "Hog_education_Hallie     65.477630    63.6825    63.69900  \n",
      "Hog_education_Haywood   139.046685   137.9175   137.91000  \n",
      "Hog_education_Janell   3365.813318  3002.0000  3017.70000  \n",
      "Hog_education_Rachael    74.879867    69.6970    69.69700  \n",
      "Hog_education_Wayne     165.057844   154.6270   154.64935  \n",
      "Robin_education_Zenia   200.968600   193.5000   193.50000  \n",
      "正在进行数据归一化...\n",
      "'Hog_lodging_Brian'列中有17543个值未通过分组归一化，使用全局归一化\n",
      "'Hog_lodging_Nikki'列中有17544个值未通过分组归一化，使用全局归一化\n",
      "'Hog_lodging_Ora'列中有17544个值未通过分组归一化，使用全局归一化\n",
      "'Robin_lodging_Celia'列中有17544个值未通过分组归一化，使用全局归一化\n",
      "'Robin_lodging_Elmer'列中有17544个值未通过分组归一化，使用全局归一化\n",
      "'Robin_lodging_Renea'列中有17544个值未通过分组归一化，使用全局归一化\n",
      "'Hog_health_Hisako'列中有17544个值未通过分组归一化，使用全局归一化\n",
      "'Hog_health_Jenny'列中有17544个值未通过分组归一化，使用全局归一化\n",
      "'Hog_health_Kesha'列中有17544个值未通过分组归一化，使用全局归一化\n",
      "'Rat_health_Gaye'列中有17544个值未通过分组归一化，使用全局归一化\n",
      "'Rat_health_Guy'列中有17536个值未通过分组归一化，使用全局归一化\n",
      "'Rat_health_Shane'列中有17544个值未通过分组归一化，使用全局归一化\n",
      "'Rat_public_Chrissy'列中有17540个值未通过分组归一化，使用全局归一化\n",
      "'Eagle_public_Pearle'列中有17117个值未通过分组归一化，使用全局归一化\n",
      "'Hog_public_Crystal'列中有17544个值未通过分组归一化，使用全局归一化\n",
      "'Hog_public_Kevin'列中有17544个值未通过分组归一化，使用全局归一化\n",
      "'Hog_public_Octavia'列中有17544个值未通过分组归一化，使用全局归一化\n",
      "'Rat_public_Roma'列中有17542个值未通过分组归一化，使用全局归一化\n",
      "'Eagle_office_Ryan'列中有17492个值未通过分组归一化，使用全局归一化\n",
      "'Rat_office_Tracy'列中有17533个值未通过分组归一化，使用全局归一化\n",
      "'Robin_office_Lindsay'列中有17544个值未通过分组归一化，使用全局归一化\n",
      "'Wolf_office_Bobbie'列中有17543个值未通过分组归一化，使用全局归一化\n",
      "'Hog_office_Sung'列中有17544个值未通过分组归一化，使用全局归一化\n",
      "'Wolf_office_Cary'列中有17544个值未通过分组归一化，使用全局归一化\n",
      "'Hog_education_Hallie'列中有17544个值未通过分组归一化，使用全局归一化\n",
      "'Hog_education_Haywood'列中有17544个值未通过分组归一化，使用全局归一化\n",
      "'Hog_education_Janell'列中有17543个值未通过分组归一化，使用全局归一化\n",
      "'Hog_education_Rachael'列中有17544个值未通过分组归一化，使用全局归一化\n",
      "'Hog_education_Wayne'列中有17544个值未通过分组归一化，使用全局归一化\n",
      "'Robin_education_Zenia'列中有17544个值未通过分组归一化，使用全局归一化\n",
      "归一化完成!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "\n",
    "# 读取数据\n",
    "file_path = 'electricity.csv'  # 根据实际情况修改\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"文件 {file_path} 不存在\")\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 检查时间戳列是否存在\n",
    "if 'timestamp' not in data.columns:\n",
    "    raise ValueError(\"数据中缺少'timestamp'列\")\n",
    "\n",
    "# 将时间列转换为datetime格式\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "\n",
    "# ===== 时间戳检查代码 =====\n",
    "# 1. 检查时间戳是否有缺失值\n",
    "missing_timestamps = data['timestamp'].isnull().sum()\n",
    "if missing_timestamps > 0:\n",
    "    print(f\"警告: 时间戳列中有 {missing_timestamps} 个缺失值\")\n",
    "\n",
    "# 2. 检查时间戳是否有重复值\n",
    "duplicate_timestamps = data['timestamp'].duplicated().sum()\n",
    "if duplicate_timestamps > 0:\n",
    "    print(f\"警告: 时间戳列中有 {duplicate_timestamps} 个重复值\")\n",
    "    duplicate_indices = data[data['timestamp'].duplicated(keep='first')].index\n",
    "    print(f\"重复的时间戳索引: {duplicate_indices.tolist()}\")\n",
    "\n",
    "# 3. 检查时间间隔是否一致\n",
    "# 先按时间戳排序\n",
    "data = data.sort_values('timestamp')\n",
    "\n",
    "# 计算时间差\n",
    "time_diff = data['timestamp'].diff()\n",
    "\n",
    "# 获取最常见的时间间隔(应该是1小时)\n",
    "expected_interval = time_diff.mode().iloc[0]\n",
    "print(f\"最常见的时间间隔: {expected_interval}\")\n",
    "\n",
    "# 找出不符合预期间隔的时间点(排除第一个点)\n",
    "irregular_intervals = time_diff[1:][time_diff[1:] != expected_interval]\n",
    "if not irregular_intervals.empty:\n",
    "    print(f\"发现 {len(irregular_intervals)} 个不规则的时间间隔\")\n",
    "    for idx, interval in irregular_intervals.items():\n",
    "        current_time = data.loc[idx, 'timestamp']\n",
    "        previous_time = data.loc[idx-1, 'timestamp'] if idx > 0 else None\n",
    "        print(f\"索引 {idx}: {previous_time} -> {current_time}, 间隔: {interval}\")\n",
    "\n",
    "# 4. 检查是否覆盖了预期的完整时间范围\n",
    "start_date = data['timestamp'].min()\n",
    "end_date = data['timestamp'].max()\n",
    "print(f\"数据时间范围: {start_date} 到 {end_date}\")\n",
    "\n",
    "# 创建理想的时间范围(每小时一个点)\n",
    "ideal_range = pd.date_range(start=start_date, end=end_date, freq='h')  # 修正为'h'\n",
    "\n",
    "# 检查是否有缺失的时间点\n",
    "missing_times = set(ideal_range) - set(data['timestamp'])\n",
    "if missing_times:\n",
    "    print(f\"发现 {len(missing_times)} 个缺失的时间点\")\n",
    "    # 仅打印前10个缺失时间点(如果太多)\n",
    "    print_times = sorted(list(missing_times))[:10]\n",
    "    print(f\"部分缺失时间点: {print_times}\")\n",
    "    if len(missing_times) > 10:\n",
    "        print(f\"... 以及 {len(missing_times) - 10} 个其他缺失时间点\")\n",
    "\n",
    "# 5. 检查是否有超出预期范围的时间点\n",
    "expected_start = pd.Timestamp('2016-01-01 00:00:00')\n",
    "expected_end = pd.Timestamp('2017-12-31 23:00:00')\n",
    "\n",
    "out_of_range = data[(data['timestamp'] < expected_start) | (data['timestamp'] > expected_end)]\n",
    "if not out_of_range.empty:\n",
    "    print(f\"发现 {len(out_of_range)} 个超出预期范围的时间点\")\n",
    "    print(out_of_range['timestamp'].tolist())\n",
    "\n",
    "# ===== 时间戳检查结束 =====\n",
    "\n",
    "# 设置时间戳为索引\n",
    "data.set_index('timestamp', inplace=True)\n",
    "\n",
    "# 选择所有数值列\n",
    "numeric_cols = data.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "if not numeric_cols:\n",
    "    raise ValueError(\"数据中没有数值列可以处理\")\n",
    "\n",
    "print(f\"将处理以下列: {numeric_cols}\")\n",
    "\n",
    "# 保存原始数据的副本，用于后续比较\n",
    "original_data = data.copy()\n",
    "\n",
    "# 1. 改进的异常值检测 - 使用更保守的方法\n",
    "def detect_outliers_improved(df, columns):\n",
    "    outliers_dict = {}\n",
    "    \n",
    "    for col in columns:\n",
    "        # 计算列的统计量 - 使用更保守的百分位数\n",
    "        Q1 = df[col].quantile(0.01)  # 更保守的下界\n",
    "        Q3 = df[col].quantile(0.99)  # 更保守的上界\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # 设置更宽松的界限\n",
    "        lower_bound = Q1 - 5 * IQR  # 使用更大乘数\n",
    "        upper_bound = Q3 + 5 * IQR\n",
    "        \n",
    "        # 找出异常值\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)].index\n",
    "        outliers_dict[col] = outliers\n",
    "        \n",
    "        print(f\"检测到{len(outliers)}个异常值在'{col}'列 (修改后的IQR方法)\")\n",
    "    \n",
    "    return outliers_dict\n",
    "\n",
    "# 2. 改进的异常值替换方法 - 避免将非零值替换为零\n",
    "def replace_outliers_improved(df, outliers_dict, original_df):\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # 确保所有数值列都是浮点型，避免类型不匹配问题\n",
    "    for col in numeric_cols:\n",
    "        df_clean[col] = df_clean[col].astype(float)\n",
    "    \n",
    "    for col, idx_list in outliers_dict.items():\n",
    "        if len(idx_list) > 0:\n",
    "            for idx in idx_list:\n",
    "                # 获取当前异常值\n",
    "                original_value = df.loc[idx, col]\n",
    "                \n",
    "                # 获取当前异常值的时间信息\n",
    "                current_hour = idx.hour\n",
    "                current_dayofweek = idx.dayofweek\n",
    "                \n",
    "                # 找出相似时间模式的非异常值\n",
    "                similar_times = []\n",
    "                \n",
    "                # 查找前后7天同一时间点的数据\n",
    "                for days_offset in range(-7, 8):\n",
    "                    if days_offset == 0:\n",
    "                        continue  # 跳过当前日期\n",
    "                    \n",
    "                    similar_time = idx + pd.Timedelta(days=days_offset)\n",
    "                    if similar_time in df.index and similar_time not in idx_list:\n",
    "                        similar_times.append(df.loc[similar_time, col])\n",
    "                \n",
    "                # 如果找到足够的相似时间点数据\n",
    "                if len(similar_times) >= 3:\n",
    "                    # 使用相似时间点的中位数替换异常值\n",
    "                    replacement_value = np.median(similar_times)\n",
    "                    \n",
    "                    # 重要：如果原始值不为零而替换值为零，保留原始值\n",
    "                    if original_value != 0 and replacement_value == 0:\n",
    "                        print(f\"保留列'{col}'中索引{idx}的非零原始值({original_value})而不是替换为0\")\n",
    "                        df_clean.loc[idx, col] = original_value\n",
    "                    else:\n",
    "                        df_clean.loc[idx, col] = replacement_value\n",
    "                else:\n",
    "                    # 如果没有足够的相似时间点，尝试使用同一小时段的数据\n",
    "                    same_hour_data = []\n",
    "                    for potential_idx in df.index:\n",
    "                        if (potential_idx.hour == current_hour and \n",
    "                            potential_idx not in idx_list and\n",
    "                            (potential_idx.dayofweek >= 5) == (current_dayofweek >= 5)):  # 工作日/周末匹配\n",
    "                            same_hour_data.append(df.loc[potential_idx, col])\n",
    "                    \n",
    "                    if len(same_hour_data) >= 3:\n",
    "                        replacement_value = np.median(same_hour_data)\n",
    "                        \n",
    "                        # 同样检查：如果原始值不为零而替换值为零，保留原始值\n",
    "                        if original_value != 0 and replacement_value == 0:\n",
    "                            print(f\"保留列'{col}'中索引{idx}的非零原始值({original_value})而不是替换为0\")\n",
    "                            df_clean.loc[idx, col] = original_value\n",
    "                        else:\n",
    "                            df_clean.loc[idx, col] = replacement_value\n",
    "                    else:\n",
    "                        # 如果仍然没有足够数据，保留原值\n",
    "                        df_clean.loc[idx, col] = original_value\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# 3. 改进的缺失值填充 - 使用相似时间模式并避免零值替换\n",
    "def fill_missing_values_improved(df, columns, original_df):\n",
    "    df_filled = df.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        # 检查是否有缺失值\n",
    "        missing_mask = df[col].isnull()\n",
    "        n_missing = missing_mask.sum()\n",
    "        \n",
    "        if n_missing > 0:\n",
    "            print(f\"'{col}'列中有{n_missing}个缺失值\")\n",
    "            \n",
    "            # 对于每个缺失值\n",
    "            for idx in df[missing_mask].index:\n",
    "                # 尝试从原始数据获取值（如果不是NaN）\n",
    "                if idx in original_df.index and not pd.isna(original_df.loc[idx, col]):\n",
    "                    original_value = original_df.loc[idx, col]\n",
    "                    df_filled.loc[idx, col] = original_value\n",
    "                    print(f\"使用原始数据中的值填充'{col}'列索引{idx}的缺失值: {original_value}\")\n",
    "                    continue\n",
    "                    \n",
    "                # 获取时间特征\n",
    "                current_hour = idx.hour\n",
    "                current_dayofweek = idx.dayofweek\n",
    "                \n",
    "                # 查找相似时间模式的有效值\n",
    "                similar_times = []\n",
    "                \n",
    "                # 尝试前后7天同一时间点的数据\n",
    "                for days_offset in range(-7, 8):\n",
    "                    if days_offset == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    similar_time = idx + pd.Timedelta(days=days_offset)\n",
    "                    if similar_time in df.index and not pd.isna(df.loc[similar_time, col]):\n",
    "                        similar_times.append(df.loc[similar_time, col])\n",
    "                \n",
    "                if len(similar_times) >= 3:\n",
    "                    # 使用相似时间点的中位数填充\n",
    "                    replacement_value = np.median(similar_times)\n",
    "                    \n",
    "                    # 确保不会将非零值替换为零\n",
    "                    # 对于缺失值，我们没有原始值可比较，所以只要确保填充值不为零即可\n",
    "                    if replacement_value == 0 and any(v != 0 for v in similar_times):\n",
    "                        # 如果中位数为零但有非零值，使用非零值的平均值\n",
    "                        non_zero_values = [v for v in similar_times if v != 0]\n",
    "                        if non_zero_values:\n",
    "                            replacement_value = np.mean(non_zero_values)\n",
    "                    \n",
    "                    df_filled.loc[idx, col] = replacement_value\n",
    "                else:\n",
    "                    # 尝试使用同一小时段的数据\n",
    "                    same_hour_data = []\n",
    "                    for potential_idx in df.index:\n",
    "                        if (potential_idx.hour == current_hour and \n",
    "                            not pd.isna(df.loc[potential_idx, col]) and\n",
    "                            (potential_idx.dayofweek >= 5) == (current_dayofweek >= 5)):\n",
    "                            same_hour_data.append(df.loc[potential_idx, col])\n",
    "                    \n",
    "                    if len(same_hour_data) >= 3:\n",
    "                        replacement_value = np.median(same_hour_data)\n",
    "                        \n",
    "                        # 同样确保不会填充零值\n",
    "                        if replacement_value == 0 and any(v != 0 for v in same_hour_data):\n",
    "                            non_zero_values = [v for v in same_hour_data if v != 0]\n",
    "                            if non_zero_values:\n",
    "                                replacement_value = np.mean(non_zero_values)\n",
    "                        \n",
    "                        df_filled.loc[idx, col] = replacement_value\n",
    "                    else:\n",
    "                        # 如果没有足够数据，使用列的非零中位数\n",
    "                        non_zero_median = df[df[col] != 0][col].median()\n",
    "                        if pd.isna(non_zero_median):  # 如果没有非零值\n",
    "                            df_filled.loc[idx, col] = df[col].median()\n",
    "                        else:\n",
    "                            df_filled.loc[idx, col] = non_zero_median\n",
    "    \n",
    "    return df_filled\n",
    "\n",
    "# 4. 最终检查函数 - 确保没有非零值被错误地替换为零\n",
    "def ensure_nonzero_preserved(df_processed, df_original, columns):\n",
    "    \"\"\"确保所有列中的非零值不会被错误地设置为零\"\"\"\n",
    "    df_fixed = df_processed.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        # 检查是否有被设为0的非零原始值\n",
    "        mask = (df_processed[col] == 0) & (df_original[col] > 0)\n",
    "        count = mask.sum()\n",
    "        \n",
    "        if count > 0:\n",
    "            print(f\"修复：发现{count}个'{col}'列中的非零值被错误地设置为0，正在恢复原值\")\n",
    "            # 恢复原始值\n",
    "            df_fixed.loc[mask, col] = df_original.loc[mask, col]\n",
    "    \n",
    "    return df_fixed\n",
    "\n",
    "# 执行数据处理流程\n",
    "# 1. 检测异常值\n",
    "outliers = detect_outliers_improved(data, numeric_cols)\n",
    "\n",
    "# 2. 替换异常值\n",
    "data_clean = replace_outliers_improved(data, outliers, original_data)\n",
    "\n",
    "# 3. 填充缺失值\n",
    "data_filled = fill_missing_values_improved(data_clean, numeric_cols, original_data)\n",
    "# 4. 最终检查 - 确保所有非零值都被保留\n",
    "data_final = ensure_nonzero_preserved(data_filled, original_data, numeric_cols)\n",
    "\n",
    "# 检查是否仍有缺失值\n",
    "missing_counts = data_final[numeric_cols].isnull().sum()\n",
    "if missing_counts.sum() > 0:\n",
    "    print(\"警告: 数据中仍存在缺失值，使用非零中位数填充\")\n",
    "    for col in numeric_cols:\n",
    "        missing_in_col = missing_counts[col]\n",
    "        if missing_in_col > 0:\n",
    "            # 使用非零中位数填充\n",
    "            non_zero_median = data_final[data_final[col] > 0][col].median()\n",
    "            if pd.isna(non_zero_median):  # 如果没有非零值\n",
    "                data_final[col] = data_final[col].fillna(data_final[col].median())\n",
    "            else:\n",
    "                data_final[col] = data_final[col].fillna(non_zero_median)\n",
    "            print(f\"填充了'{col}'列中的{missing_in_col}个缺失值\")\n",
    "\n",
    "# 5. 数据类型修复 - 确保没有类型不匹配警告\n",
    "for col in numeric_cols:\n",
    "    # 将所有数值列转换为浮点型\n",
    "    data_final[col] = data_final[col].astype(float)\n",
    "\n",
    "# 保存处理后的数据\n",
    "data_final.to_csv('electricity_cleaned.csv')\n",
    "\n",
    "print(\"数据处理完成!\")\n",
    "\n",
    "# 验证处理结果\n",
    "def validate_processing(original, processed, columns):\n",
    "    \"\"\"验证处理结果，确保没有非零值被错误地设置为零\"\"\"\n",
    "    all_valid = True\n",
    "    \n",
    "    for col in columns:\n",
    "        # 检查是否有非零值被设为零\n",
    "        zero_mask = (processed[col] == 0) & (original[col] > 0)\n",
    "        zero_count = zero_mask.sum()\n",
    "        \n",
    "        if zero_count > 0:\n",
    "            print(f\"警告: 列'{col}'中有{zero_count}个非零值被错误地设置为0\")\n",
    "            all_valid = False\n",
    "            \n",
    "            # 显示部分被错误修改的值\n",
    "            if zero_count > 5:\n",
    "                sample_indices = original[zero_mask].index[:5]\n",
    "                print(\"部分示例:\")\n",
    "                for idx in sample_indices:\n",
    "                    print(f\"  索引 {idx}: 原值 {original.loc[idx, col]} -> 处理后 {processed.loc[idx, col]}\")\n",
    "                print(f\"  ...以及{zero_count-5}个其他值\")\n",
    "            else:\n",
    "                for idx in original[zero_mask].index:\n",
    "                    print(f\"  索引 {idx}: 原值 {original.loc[idx, col]} -> 处理后 {processed.loc[idx, col]}\")\n",
    "    \n",
    "    if all_valid:\n",
    "        print(\"验证通过: 所有列中的非零值都被正确保留\")\n",
    "    \n",
    "    return all_valid\n",
    "\n",
    "# 验证处理结果\n",
    "validation_result = validate_processing(original_data, data_final, numeric_cols)\n",
    "\n",
    "# 提供处理前后的统计比较\n",
    "print(\"\\n数据处理前后统计比较:\")\n",
    "comparison_stats = pd.DataFrame({\n",
    "    '原始数据缺失值': original_data[numeric_cols].isnull().sum(),\n",
    "    '处理后缺失值': data_final[numeric_cols].isnull().sum(),\n",
    "    '原始数据零值数量': (original_data[numeric_cols] == 0).sum(),\n",
    "    '处理后零值数量': (data_final[numeric_cols] == 0).sum(),\n",
    "    '原始数据均值': original_data[numeric_cols].mean(),\n",
    "    '处理后均值': data_final[numeric_cols].mean(),\n",
    "    '原始数据中位数': original_data[numeric_cols].median(),\n",
    "    '处理后中位数': data_final[numeric_cols].median()\n",
    "})\n",
    "print(comparison_stats)\n",
    "# 6. 改进的归一化方法 - 按时间特征分组归一化\n",
    "def grouped_minmax_scaling(df, columns):\n",
    "    \"\"\"按时间特征分组进行Min-Max归一化，保留非零值的特性\"\"\"\n",
    "    result = df.copy()\n",
    "    scalers = {}  # 存储每个组的缩放器\n",
    "    \n",
    "    # 按工作日/周末和小时分组归一化\n",
    "    for is_weekend in [0, 1]:  # 0=工作日, 1=周末\n",
    "        for hour in range(24):  # 0-23小时\n",
    "            # 创建分组掩码\n",
    "            mask = (df.index.dayofweek >= 5) == (is_weekend == 1)  # 工作日/周末\n",
    "            mask &= (df.index.hour == hour)  # 小时\n",
    "            \n",
    "            # 确保有足够的数据点进行归一化\n",
    "            if mask.sum() >= 5:  # 至少需要5个数据点\n",
    "                for col in columns:\n",
    "                    # 获取当前组的数据\n",
    "                    group_data = df.loc[mask, col].values.reshape(-1, 1)\n",
    "                    \n",
    "                    # 检查组内是否有足够的非零值\n",
    "                    non_zero_count = (group_data != 0).sum()\n",
    "                    if non_zero_count < 3:  # 如果非零值太少，跳过归一化\n",
    "                        print(f\"跳过'{col}'列在{hour}时{'周末' if is_weekend else '工作日'}的归一化 - 非零值不足\")\n",
    "                        continue\n",
    "                    \n",
    "                    # 创建并拟合缩放器\n",
    "                    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "                    scaler.fit(group_data)\n",
    "                    \n",
    "                    # 存储缩放器\n",
    "                    key = f\"{col}_{is_weekend}_{hour}\"\n",
    "                    scalers[key] = scaler\n",
    "                    \n",
    "                    # 应用缩放\n",
    "                    scaled_values = scaler.transform(group_data).flatten()\n",
    "                    \n",
    "                    # 重要：确保零值仍然是零\n",
    "                    zero_mask = (df.loc[mask, col].values == 0)\n",
    "                    scaled_values[zero_mask] = 0\n",
    "                    \n",
    "                    # 更新结果\n",
    "                    result.loc[mask, col] = scaled_values\n",
    "            else:\n",
    "                print(f\"跳过{hour}时{'周末' if is_weekend else '工作日'}的归一化 - 数据点不足\")\n",
    "    \n",
    "    # 处理未归一化的值\n",
    "    for col in columns:\n",
    "        # 检查是否有未归一化的值\n",
    "        not_scaled = pd.isna(result[col]) | (df[col] != result[col])\n",
    "        if not_scaled.any():\n",
    "            print(f\"'{col}'列中有{not_scaled.sum()}个值未通过分组归一化，使用全局归一化\")\n",
    "            \n",
    "            # 获取已经归一化和未归一化的索引\n",
    "            scaled_idx = ~not_scaled\n",
    "            unscaled_idx = not_scaled\n",
    "            \n",
    "            if unscaled_idx.sum() > 0:\n",
    "                # 创建全局缩放器\n",
    "                all_data = df[col].values.reshape(-1, 1)\n",
    "                global_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "                global_scaler.fit(all_data)\n",
    "                \n",
    "                # 应用全局缩放到未归一化的值\n",
    "                unscaled_data = df.loc[unscaled_idx, col].values.reshape(-1, 1)\n",
    "                scaled_values = global_scaler.transform(unscaled_data).flatten()\n",
    "                \n",
    "                # 确保零值仍然是零\n",
    "                zero_mask = (df.loc[unscaled_idx, col].values == 0)\n",
    "                scaled_values[zero_mask] = 0\n",
    "                \n",
    "                # 更新结果\n",
    "                result.loc[unscaled_idx, col] = scaled_values\n",
    "    \n",
    "    return result\n",
    "\n",
    "# 应用归一化\n",
    "print(\"正在进行数据归一化...\")\n",
    "data_normalized = grouped_minmax_scaling(data_final, numeric_cols)\n",
    "\n",
    "# 最终再次检查，确保零值保持为零\n",
    "for col in numeric_cols:\n",
    "    # 确保原始数据中的零值在归一化后仍然是零\n",
    "    zero_mask = (data_final[col] == 0)\n",
    "    if zero_mask.any():\n",
    "        if not (data_normalized.loc[zero_mask, col] == 0).all():\n",
    "            print(f\"修复: 在'{col}'列中的零值在归一化后变为非零\")\n",
    "            data_normalized.loc[zero_mask, col] = 0\n",
    "\n",
    "# 保存归一化后的数据\n",
    "data_normalized.to_csv('electricity_normalized.csv')\n",
    "print(\"归一化完成!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "730d1577-f7d3-4185-b3c9-148cdcdedf87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "将时间戳列转换为datetime格式...\n",
      "✅ 保存文件：electricity_train_data/electricity_extreme_train.csv\n",
      "✅ 保存文件：electricity_train_data/electricity_extreme_test.csv\n",
      "✅ 保存文件：electricity_train_data/electricity_heavy_train.csv\n",
      "✅ 保存文件：electricity_train_data/electricity_heavy_test.csv\n",
      "✅ 保存文件：electricity_train_data/electricity_mild_train.csv\n",
      "✅ 保存文件：electricity_train_data/electricity_mild_test.csv\n",
      "\n",
      "✅ 所有电力数据拆分完成！\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 创建输出目录\n",
    "os.makedirs('electricity_train_data', exist_ok=True)\n",
    "\n",
    "# 定义时间范围\n",
    "splits = {\n",
    "    'extreme_train': ('2016-01-01 00:00:00', '2016-01-07 23:00:00'),\n",
    "    'extreme_test': ('2016-01-08 00:00:00', '2016-12-31 23:00:00'),\n",
    "    'heavy_train': ('2016-01-01 00:00:00', '2016-01-28 23:00:00'),\n",
    "    'heavy_test': ('2016-01-29 00:00:00', '2016-12-31 23:00:00'),\n",
    "    'mild_train': ('2016-01-01 00:00:00', '2016-03-31 23:00:00'),\n",
    "    'mild_test': ('2016-04-01 00:00:00', '2016-12-31 23:00:00')\n",
    "}\n",
    "\n",
    "# 检查输入文件是否存在\n",
    "file_path = 'electricity_source_data/electricity_processed.csv'\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"输入文件不存在: {file_path}\")\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 检查时间戳列是否存在\n",
    "if 'timestamp' not in df.columns:\n",
    "    raise ValueError(\"数据中缺少'timestamp'列\")\n",
    "\n",
    "# 确保时间戳列是datetime格式\n",
    "if not pd.api.types.is_datetime64_any_dtype(df['timestamp']):\n",
    "    print(\"将时间戳列转换为datetime格式...\")\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# 拆分并保存数据\n",
    "for split_name, (start, end) in splits.items():\n",
    "    start_time = pd.Timestamp(start)\n",
    "    end_time = pd.Timestamp(end)\n",
    "\n",
    "    mask = (df['timestamp'] >= start_time) & (df['timestamp'] <= end_time)\n",
    "    split_df = df[mask].copy()\n",
    "\n",
    "    expected_len = int((end_time - start_time).total_seconds() / 3600) + 1\n",
    "    if len(split_df) != expected_len:\n",
    "        print(f\"⚠️ {split_name}：期望 {expected_len} 行，实际 {len(split_df)} 行\")\n",
    "        # 输出缺失的时间戳\n",
    "        all_timestamps = pd.date_range(start=start_time, end=end_time, freq='h')\n",
    "        missing_timestamps = set(all_timestamps) - set(df['timestamp'])\n",
    "        if missing_timestamps:\n",
    "            print(f\"   缺失的时间戳数量: {len(missing_timestamps)}\")\n",
    "            if len(missing_timestamps) <= 10:\n",
    "                print(f\"   缺失的时间戳: {sorted(missing_timestamps)}\")\n",
    "            else:\n",
    "                print(f\"   部分缺失的时间戳: {sorted(list(missing_timestamps))[:10]}...\")\n",
    "\n",
    "    split_df.to_csv(f'electricity_train_data/electricity_{split_name}.csv', index=False)\n",
    "    print(f\"✅ 保存文件：electricity_train_data/electricity_{split_name}.csv\")\n",
    "\n",
    "print(\"\\n✅ 所有电力数据拆分完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2599b1-cac8-4780-a2c4-460747605f91",
   "metadata": {},
   "source": [
    "## 天气"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c723a15-14bc-44a9-9249-84e085c5cdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已删除以下列: ['cloudCoverage', 'precipDepth1HR', 'precipDepth6HR']\n",
      "已保存Hog的天气数据，共17542行，文件名: Hog.csv\n",
      "已保存Robin的天气数据，共17516行，文件名: Robin.csv\n",
      "已保存Rat的天气数据，共17539行，文件名: Rat.csv\n",
      "已保存Eagle的天气数据，共17536行，文件名: Eagle.csv\n",
      "已保存Wolf的天气数据，共17505行，文件名: Wolf.csv\n",
      "已保存所有筛选地点的合并数据，共87638行，文件名: filtered_weather_all.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv('weather.csv')\n",
    "\n",
    "# 定义要保留的地点列表\n",
    "locations_to_keep = ['Hog', 'Robin', 'Rat', 'Eagle', 'Wolf']\n",
    "\n",
    "# 定义要删除的列名\n",
    "columns_to_drop = ['cloudCoverage', 'precipDepth1HR', 'precipDepth6HR']\n",
    "\n",
    "# 检查要删除的列是否存在于数据中\n",
    "columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "\n",
    "# 筛选数据并删除指定的列\n",
    "filtered_df = df[df['site_id'].isin(locations_to_keep)]\n",
    "filtered_df = filtered_df.drop(columns=columns_to_drop)\n",
    "\n",
    "print(f\"已删除以下列: {columns_to_drop}\")\n",
    "\n",
    "# 按地点分割数据并保存为不同的CSV文件\n",
    "for location in locations_to_keep:\n",
    "    # 提取当前地点的数据\n",
    "    location_df = filtered_df[filtered_df['site_id'] == location]\n",
    "    \n",
    "    # 跳过没有数据的地点\n",
    "    if len(location_df) == 0:\n",
    "        print(f\"警告: 没有找到'{location}'的数据\")\n",
    "        continue\n",
    "    \n",
    "    # 保存为CSV文件\n",
    "    output_file = f'{location}.csv'\n",
    "    location_df.to_csv(output_file, index=False)\n",
    "    print(f\"已保存{location}的天气数据，共{len(location_df)}行，文件名: {output_file}\")\n",
    "\n",
    "# 同时保存一个包含所有筛选地点的合并文件\n",
    "filtered_df.to_csv('filtered_weather_all.csv', index=False)\n",
    "print(f\"已保存所有筛选地点的合并数据，共{len(filtered_df)}行，文件名: filtered_weather_all.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc3d1fa-6a29-4543-8856-af03d77080df",
   "metadata": {},
   "source": [
    "### 这是天气的处理，缺值归一化等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19126a07-a968-4f06-aa44-ca17ddb8306d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 开始处理 Robin.csv ===\n",
      "成功读取文件，包含 17516 行数据\n",
      "=== 时间戳完整性检查 ===\n",
      "警告: 时间戳列中有 2 个重复值\n",
      "最常见的时间间隔: 0 days 01:00:00\n",
      "发现 19 个不规则的时间间隔\n",
      "数据时间范围: 2016-01-01 00:00:00 到 2017-12-31 23:00:00\n",
      "发现 30 个缺失的时间点\n",
      "2016年2月29日(闰年)数据完整，共有24小时的数据\n",
      "正在处理重复的时间戳...\n",
      "正在处理缺失的时间点...\n",
      "删除site_id列...\n",
      "\n",
      "处理以下列: ['airTemperature', 'dewTemperature', 'seaLvlPressure', 'windDirection', 'windSpeed']\n",
      "\n",
      "各列缺失值数量:\n",
      "airTemperature    31\n",
      "dewTemperature    31\n",
      "seaLvlPressure    91\n",
      "windDirection     36\n",
      "windSpeed         30\n",
      "dtype: int64\n",
      "\n",
      "=== 开始数据处理 ===\n",
      "\n",
      "检测异常值...\n",
      "检测到11个异常值在'airTemperature'列 (包括11个可疑的零值)\n",
      "检测到58个异常值在'dewTemperature'列 (包括58个可疑的零值)\n",
      "检测到0个异常值在'seaLvlPressure'列 (包括0个可疑的零值)\n",
      "检测到40个异常值在'windDirection'列 (包括40个可疑的零值)\n",
      "检测到40个异常值在'windSpeed'列 (包括40个可疑的零值)\n",
      "\n",
      "替换异常值...\n",
      "列'airTemperature'中替换了11个异常值\n",
      "列'dewTemperature'中替换了58个异常值\n",
      "列'windDirection'中替换了40个异常值\n",
      "列'windSpeed'中替换了40个异常值\n",
      "总共替换了149个异常值\n",
      "\n",
      "填充缺失值...\n",
      "'airTemperature'列中有31个缺失值\n",
      "列'airTemperature'中填充了31个缺失值\n",
      "'dewTemperature'列中有31个缺失值\n",
      "列'dewTemperature'中填充了31个缺失值\n",
      "'seaLvlPressure'列中有91个缺失值\n",
      "列'seaLvlPressure'中填充了91个缺失值\n",
      "'windDirection'列中有36个缺失值\n",
      "列'windDirection'中填充了36个缺失值\n",
      "'windSpeed'列中有30个缺失值\n",
      "列'windSpeed'中填充了30个缺失值\n",
      "总共填充了219个缺失值\n",
      "\n",
      "执行最终检查...\n",
      "没有发现被错误设置为0的非零值\n",
      "\n",
      "执行数据归一化...\n",
      "\n",
      "=== 执行数据归一化（全面处理） ===\n",
      "列 'airTemperature': 范围=[-4.1000, 33.9000], 零值数量=0, 含负值=True\n",
      "归一化列 'airTemperature' (包含负值): 范围从 [-4.1000, 33.9000] 归一化到 [0, 1]\n",
      "列 'dewTemperature': 范围=[-8.9000, 20.7000], 零值数量=1, 含负值=True\n",
      "归一化列 'dewTemperature' (包含负值): 范围从 [-8.9000, 20.7000] 归一化到 [0, 1]\n",
      "列 'seaLvlPressure': 范围=[972.0000, 1045.5000], 零值数量=0, 含负值=False\n",
      "归一化列 'seaLvlPressure' (保留零值): 非零值范围从 [972.0000, 1045.5000] 归一化到 [0.01, 1]\n",
      "列 'windDirection': 范围=[10.0000, 360.0000], 零值数量=0, 含负值=False\n",
      "归一化列 'windDirection' (保留零值): 非零值范围从 [10.0000, 360.0000] 归一化到 [0.01, 1]\n",
      "列 'windSpeed': 范围=[0.5000, 18.5000], 零值数量=0, 含负值=False\n",
      "归一化列 'windSpeed' (保留零值): 非零值范围从 [0.5000, 18.5000] 归一化到 [0.01, 1]\n",
      "\n",
      "=== 数据处理完成 ===\n",
      "原始数据中缺失值总数: 219\n",
      "处理后数据中缺失值总数: 0\n",
      "原始数据中0值总数: 149\n",
      "处理后数据中0值总数: 2\n",
      "\n",
      "=== 验证处理结果 ===\n",
      "验证通过: 所有非零值都被正确保留，没有缺失值\n",
      "\n",
      "=== 划分并保存数据集 ===\n",
      "划分extreme_train数据集，包含168行数据\n",
      "已保存到weather_train/Robin_extreme_train.csv\n",
      "划分extreme_test数据集，包含8616行数据\n",
      "已保存到weather_train/Robin_extreme_test.csv\n",
      "划分heavy_train数据集，包含672行数据\n",
      "已保存到weather_train/Robin_heavy_train.csv\n",
      "划分heavy_test数据集，包含8112行数据\n",
      "已保存到weather_train/Robin_heavy_test.csv\n",
      "划分mild_train数据集，包含2184行数据\n",
      "已保存到weather_train/Robin_mild_train.csv\n",
      "划分mild_test数据集，包含6600行数据\n",
      "已保存到weather_train/Robin_mild_test.csv\n",
      "\n",
      "=== 开始处理 Rat.csv ===\n",
      "成功读取文件，包含 17539 行数据\n",
      "=== 时间戳完整性检查 ===\n",
      "警告: 时间戳列中有 2 个重复值\n",
      "最常见的时间间隔: 0 days 01:00:00\n",
      "发现 9 个不规则的时间间隔\n",
      "数据时间范围: 2016-01-01 00:00:00 到 2017-12-31 23:00:00\n",
      "发现 7 个缺失的时间点\n",
      "2016年2月29日(闰年)数据完整，共有24小时的数据\n",
      "正在处理重复的时间戳...\n",
      "正在处理缺失的时间点...\n",
      "删除site_id列...\n",
      "\n",
      "处理以下列: ['airTemperature', 'dewTemperature', 'seaLvlPressure', 'windDirection', 'windSpeed']\n",
      "\n",
      "各列缺失值数量:\n",
      "airTemperature     13\n",
      "dewTemperature     16\n",
      "seaLvlPressure    307\n",
      "windDirection     297\n",
      "windSpeed          18\n",
      "dtype: int64\n",
      "\n",
      "=== 开始数据处理 ===\n",
      "\n",
      "检测异常值...\n",
      "检测到151个异常值在'airTemperature'列 (包括151个可疑的零值)\n",
      "检测到237个异常值在'dewTemperature'列 (包括237个可疑的零值)\n",
      "检测到0个异常值在'seaLvlPressure'列 (包括0个可疑的零值)\n",
      "检测到1215个异常值在'windDirection'列 (包括1215个可疑的零值)\n",
      "检测到1215个异常值在'windSpeed'列 (包括1215个可疑的零值)\n",
      "\n",
      "替换异常值...\n",
      "列'airTemperature'中替换了151个异常值\n",
      "列'dewTemperature'中替换了237个异常值\n",
      "列'windDirection'中替换了1215个异常值\n",
      "列'windSpeed'中替换了1215个异常值\n",
      "总共替换了2818个异常值\n",
      "\n",
      "填充缺失值...\n",
      "'airTemperature'列中有13个缺失值\n",
      "列'airTemperature'中填充了13个缺失值\n",
      "'dewTemperature'列中有16个缺失值\n",
      "列'dewTemperature'中填充了16个缺失值\n",
      "'seaLvlPressure'列中有307个缺失值\n",
      "列'seaLvlPressure'中填充了307个缺失值\n",
      "'windDirection'列中有297个缺失值\n",
      "列'windDirection'中填充了297个缺失值\n",
      "'windSpeed'列中有18个缺失值\n",
      "列'windSpeed'中填充了18个缺失值\n",
      "总共填充了651个缺失值\n",
      "\n",
      "执行最终检查...\n",
      "没有发现被错误设置为0的非零值\n",
      "\n",
      "执行数据归一化...\n",
      "\n",
      "=== 执行数据归一化（全面处理） ===\n",
      "列 'airTemperature': 范围=[-10.6000, 37.8000], 零值数量=2, 含负值=True\n",
      "归一化列 'airTemperature' (包含负值): 范围从 [-10.6000, 37.8000] 归一化到 [0, 1]\n",
      "列 'dewTemperature': 范围=[-22.8000, 26.7000], 零值数量=4, 含负值=True\n",
      "归一化列 'dewTemperature' (包含负值): 范围从 [-22.8000, 26.7000] 归一化到 [0, 1]\n",
      "列 'seaLvlPressure': 范围=[990.5000, 1043.0000], 零值数量=0, 含负值=False\n",
      "归一化列 'seaLvlPressure' (保留零值): 非零值范围从 [990.5000, 1043.0000] 归一化到 [0.01, 1]\n",
      "列 'windDirection': 范围=[10.0000, 360.0000], 零值数量=0, 含负值=False\n",
      "归一化列 'windDirection' (保留零值): 非零值范围从 [10.0000, 360.0000] 归一化到 [0.01, 1]\n",
      "列 'windSpeed': 范围=[1.5000, 17.0000], 零值数量=0, 含负值=False\n",
      "归一化列 'windSpeed' (保留零值): 非零值范围从 [1.5000, 17.0000] 归一化到 [0.01, 1]\n",
      "\n",
      "=== 数据处理完成 ===\n",
      "原始数据中缺失值总数: 651\n",
      "处理后数据中缺失值总数: 0\n",
      "原始数据中0值总数: 2818\n",
      "处理后数据中0值总数: 3\n",
      "\n",
      "=== 验证处理结果 ===\n",
      "验证通过: 所有非零值都被正确保留，没有缺失值\n",
      "\n",
      "=== 划分并保存数据集 ===\n",
      "划分extreme_train数据集，包含168行数据\n",
      "已保存到weather_train/Rat_extreme_train.csv\n",
      "划分extreme_test数据集，包含8616行数据\n",
      "已保存到weather_train/Rat_extreme_test.csv\n",
      "划分heavy_train数据集，包含672行数据\n",
      "已保存到weather_train/Rat_heavy_train.csv\n",
      "划分heavy_test数据集，包含8112行数据\n",
      "已保存到weather_train/Rat_heavy_test.csv\n",
      "划分mild_train数据集，包含2184行数据\n",
      "已保存到weather_train/Rat_mild_train.csv\n",
      "划分mild_test数据集，包含6600行数据\n",
      "已保存到weather_train/Rat_mild_test.csv\n",
      "\n",
      "=== 开始处理 Eagle.csv ===\n",
      "成功读取文件，包含 17536 行数据\n",
      "=== 时间戳完整性检查 ===\n",
      "警告: 时间戳列中有 2 个重复值\n",
      "最常见的时间间隔: 0 days 01:00:00\n",
      "发现 7 个不规则的时间间隔\n",
      "数据时间范围: 2016-01-01 00:00:00 到 2017-12-31 23:00:00\n",
      "发现 10 个缺失的时间点\n",
      "2016年2月29日(闰年)数据完整，共有24小时的数据\n",
      "正在处理重复的时间戳...\n",
      "正在处理缺失的时间点...\n",
      "删除site_id列...\n",
      "\n",
      "处理以下列: ['airTemperature', 'dewTemperature', 'seaLvlPressure', 'windDirection', 'windSpeed']\n",
      "\n",
      "各列缺失值数量:\n",
      "airTemperature     13\n",
      "dewTemperature     13\n",
      "seaLvlPressure    217\n",
      "windDirection     536\n",
      "windSpeed          48\n",
      "dtype: int64\n",
      "\n",
      "=== 开始数据处理 ===\n",
      "\n",
      "检测异常值...\n",
      "检测到209个异常值在'airTemperature'列 (包括209个可疑的零值)\n",
      "检测到290个异常值在'dewTemperature'列 (包括290个可疑的零值)\n",
      "检测到0个异常值在'seaLvlPressure'列 (包括0个可疑的零值)\n",
      "检测到2392个异常值在'windDirection'列 (包括2392个可疑的零值)\n",
      "检测到2392个异常值在'windSpeed'列 (包括2392个可疑的零值)\n",
      "\n",
      "替换异常值...\n",
      "列'airTemperature'中替换了209个异常值\n",
      "列'dewTemperature'中替换了290个异常值\n",
      "列'windDirection'中替换了2392个异常值\n",
      "列'windSpeed'中替换了2392个异常值\n",
      "总共替换了5283个异常值\n",
      "\n",
      "填充缺失值...\n",
      "'airTemperature'列中有13个缺失值\n",
      "列'airTemperature'中填充了13个缺失值\n",
      "'dewTemperature'列中有13个缺失值\n",
      "列'dewTemperature'中填充了13个缺失值\n",
      "'seaLvlPressure'列中有217个缺失值\n",
      "列'seaLvlPressure'中填充了217个缺失值\n",
      "'windDirection'列中有536个缺失值\n",
      "列'windDirection'中填充了536个缺失值\n",
      "'windSpeed'列中有48个缺失值\n",
      "列'windSpeed'中填充了48个缺失值\n",
      "总共填充了827个缺失值\n",
      "\n",
      "执行最终检查...\n",
      "没有发现被错误设置为0的非零值\n",
      "\n",
      "执行数据归一化...\n",
      "\n",
      "=== 执行数据归一化（全面处理） ===\n",
      "列 'airTemperature': 范围=[-15.6000, 35.6000], 零值数量=4, 含负值=True\n",
      "归一化列 'airTemperature' (包含负值): 范围从 [-15.6000, 35.6000] 归一化到 [0, 1]\n",
      "列 'dewTemperature': 范围=[-25.6000, 25.6000], 零值数量=13, 含负值=True\n",
      "归一化列 'dewTemperature' (包含负值): 范围从 [-25.6000, 25.6000] 归一化到 [0, 1]\n",
      "列 'seaLvlPressure': 范围=[982.6000, 1041.6000], 零值数量=0, 含负值=False\n",
      "归一化列 'seaLvlPressure' (保留零值): 非零值范围从 [982.6000, 1041.6000] 归一化到 [0.01, 1]\n",
      "列 'windDirection': 范围=[10.0000, 360.0000], 零值数量=0, 含负值=False\n",
      "归一化列 'windDirection' (保留零值): 非零值范围从 [10.0000, 360.0000] 归一化到 [0.01, 1]\n",
      "列 'windSpeed': 范围=[1.5000, 13.4000], 零值数量=0, 含负值=False\n",
      "归一化列 'windSpeed' (保留零值): 非零值范围从 [1.5000, 13.4000] 归一化到 [0.01, 1]\n",
      "\n",
      "=== 数据处理完成 ===\n",
      "原始数据中缺失值总数: 827\n",
      "处理后数据中缺失值总数: 0\n",
      "原始数据中0值总数: 5283\n",
      "处理后数据中0值总数: 3\n",
      "\n",
      "=== 验证处理结果 ===\n",
      "验证通过: 所有非零值都被正确保留，没有缺失值\n",
      "\n",
      "=== 划分并保存数据集 ===\n",
      "划分extreme_train数据集，包含168行数据\n",
      "已保存到weather_train/Eagle_extreme_train.csv\n",
      "划分extreme_test数据集，包含8616行数据\n",
      "已保存到weather_train/Eagle_extreme_test.csv\n",
      "划分heavy_train数据集，包含672行数据\n",
      "已保存到weather_train/Eagle_heavy_train.csv\n",
      "划分heavy_test数据集，包含8112行数据\n",
      "已保存到weather_train/Eagle_heavy_test.csv\n",
      "划分mild_train数据集，包含2184行数据\n",
      "已保存到weather_train/Eagle_mild_train.csv\n",
      "划分mild_test数据集，包含6600行数据\n",
      "已保存到weather_train/Eagle_mild_test.csv\n",
      "\n",
      "=== 开始处理 Wolf.csv ===\n",
      "成功读取文件，包含 17505 行数据\n",
      "=== 时间戳完整性检查 ===\n",
      "警告: 时间戳列中有 2 个重复值\n",
      "最常见的时间间隔: 0 days 01:00:00\n",
      "发现 24 个不规则的时间间隔\n",
      "数据时间范围: 2016-01-01 00:00:00 到 2017-12-31 23:00:00\n",
      "发现 41 个缺失的时间点\n",
      "2016年2月29日(闰年)数据完整，共有24小时的数据\n",
      "正在处理重复的时间戳...\n",
      "正在处理缺失的时间点...\n",
      "删除site_id列...\n",
      "\n",
      "处理以下列: ['airTemperature', 'dewTemperature', 'seaLvlPressure', 'windDirection', 'windSpeed']\n",
      "\n",
      "各列缺失值数量:\n",
      "airTemperature     41\n",
      "dewTemperature     41\n",
      "seaLvlPressure    107\n",
      "windDirection      43\n",
      "windSpeed          41\n",
      "dtype: int64\n",
      "\n",
      "=== 开始数据处理 ===\n",
      "\n",
      "检测异常值...\n",
      "检测到21个异常值在'airTemperature'列 (包括21个可疑的零值)\n",
      "检测到73个异常值在'dewTemperature'列 (包括73个可疑的零值)\n",
      "检测到0个异常值在'seaLvlPressure'列 (包括0个可疑的零值)\n",
      "检测到96个异常值在'windDirection'列 (包括96个可疑的零值)\n",
      "检测到96个异常值在'windSpeed'列 (包括96个可疑的零值)\n",
      "\n",
      "替换异常值...\n",
      "列'airTemperature'中替换了21个异常值\n",
      "列'dewTemperature'中替换了73个异常值\n",
      "列'windDirection'中替换了96个异常值\n",
      "列'windSpeed'中替换了96个异常值\n",
      "总共替换了286个异常值\n",
      "\n",
      "填充缺失值...\n",
      "'airTemperature'列中有41个缺失值\n",
      "列'airTemperature'中填充了41个缺失值\n",
      "'dewTemperature'列中有41个缺失值\n",
      "列'dewTemperature'中填充了41个缺失值\n",
      "'seaLvlPressure'列中有107个缺失值\n",
      "列'seaLvlPressure'中填充了107个缺失值\n",
      "'windDirection'列中有43个缺失值\n",
      "列'windDirection'中填充了43个缺失值\n",
      "'windSpeed'列中有41个缺失值\n",
      "列'windSpeed'中填充了41个缺失值\n",
      "总共填充了273个缺失值\n",
      "\n",
      "执行最终检查...\n",
      "没有发现被错误设置为0的非零值\n",
      "\n",
      "执行数据归一化...\n",
      "\n",
      "=== 执行数据归一化（全面处理） ===\n",
      "列 'airTemperature': 范围=[-4.8000, 26.1000], 零值数量=0, 含负值=True\n",
      "归一化列 'airTemperature' (包含负值): 范围从 [-4.8000, 26.1000] 归一化到 [0, 1]\n",
      "列 'dewTemperature': 范围=[-5.7000, 18.3000], 零值数量=0, 含负值=True\n",
      "归一化列 'dewTemperature' (包含负值): 范围从 [-5.7000, 18.3000] 归一化到 [0, 1]\n",
      "列 'seaLvlPressure': 范围=[968.2000, 1041.9000], 零值数量=0, 含负值=False\n",
      "归一化列 'seaLvlPressure' (保留零值): 非零值范围从 [968.2000, 1041.9000] 归一化到 [0.01, 1]\n",
      "列 'windDirection': 范围=[10.0000, 360.0000], 零值数量=0, 含负值=False\n",
      "归一化列 'windDirection' (保留零值): 非零值范围从 [10.0000, 360.0000] 归一化到 [0.01, 1]\n",
      "列 'windSpeed': 范围=[0.5000, 23.0000], 零值数量=0, 含负值=False\n",
      "归一化列 'windSpeed' (保留零值): 非零值范围从 [0.5000, 23.0000] 归一化到 [0.01, 1]\n",
      "\n",
      "=== 数据处理完成 ===\n",
      "原始数据中缺失值总数: 273\n",
      "处理后数据中缺失值总数: 0\n",
      "原始数据中0值总数: 286\n",
      "处理后数据中0值总数: 2\n",
      "\n",
      "=== 验证处理结果 ===\n",
      "验证通过: 所有非零值都被正确保留，没有缺失值\n",
      "\n",
      "=== 划分并保存数据集 ===\n",
      "划分extreme_train数据集，包含168行数据\n",
      "已保存到weather_train/Wolf_extreme_train.csv\n",
      "划分extreme_test数据集，包含8616行数据\n",
      "已保存到weather_train/Wolf_extreme_test.csv\n",
      "划分heavy_train数据集，包含672行数据\n",
      "已保存到weather_train/Wolf_heavy_train.csv\n",
      "划分heavy_test数据集，包含8112行数据\n",
      "已保存到weather_train/Wolf_heavy_test.csv\n",
      "划分mild_train数据集，包含2184行数据\n",
      "已保存到weather_train/Wolf_mild_train.csv\n",
      "划分mild_test数据集，包含6600行数据\n",
      "已保存到weather_train/Wolf_mild_test.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 定义要处理的文件列表\n",
    "file_names = [ 'Robin', 'Rat', 'Eagle', 'Wolf']\n",
    "\n",
    "# 定义不同数据集的时间范围\n",
    "dataset_types = {\n",
    "    'extreme_train': ('2016-01-01 00:00:00', '2016-01-07 23:00:00'),\n",
    "    'extreme_test': ('2016-01-08 00:00:00', '2016-12-31 23:00:00'),\n",
    "    'heavy_train': ('2016-01-01 00:00:00', '2016-01-28 23:00:00'),\n",
    "    'heavy_test': ('2016-01-29 00:00:00', '2016-12-31 23:00:00'),\n",
    "    'mild_train': ('2016-01-01 00:00:00', '2016-03-31 23:00:00'),\n",
    "    'mild_test': ('2016-04-01 00:00:00', '2016-12-31 23:00:00')\n",
    "}\n",
    "\n",
    "# 确保输出目录存在\n",
    "output_dir = 'weather_train'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 控制详细日志输出\n",
    "VERBOSE = False\n",
    "\n",
    "# 为每个文件执行处理\n",
    "for file_base in file_names:\n",
    "    file_path = f'{file_base}.csv'  # 构建完整的文件路径\n",
    "    \n",
    "    print(f\"\\n=== 开始处理 {file_path} ===\")\n",
    "    \n",
    "    # 检查文件是否存在\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"警告: 文件 {file_path} 不存在，跳过处理\")\n",
    "        continue\n",
    "    \n",
    "    # 读取CSV文件\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        print(f\"成功读取文件，包含 {len(data)} 行数据\")\n",
    "    except Exception as e:\n",
    "        print(f\"读取文件 {file_path} 时出错: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # 将时间列转换为datetime格式\n",
    "    if 'timestamp' in data.columns:\n",
    "        data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "    else:\n",
    "        print(f\"错误: {file_path} 中缺少'timestamp'列\")\n",
    "        continue\n",
    "    \n",
    "    # ===== 时间戳检查代码 =====\n",
    "    print(\"=== 时间戳完整性检查 ===\")\n",
    "    \n",
    "    # 1. 检查时间戳是否有缺失值\n",
    "    missing_timestamps = data['timestamp'].isnull().sum()\n",
    "    if missing_timestamps > 0:\n",
    "        print(f\"警告: 时间戳列中有 {missing_timestamps} 个缺失值\")\n",
    "    \n",
    "    # 2. 检查时间戳是否有重复值\n",
    "    duplicate_timestamps = data['timestamp'].duplicated().sum()\n",
    "    if duplicate_timestamps > 0:\n",
    "        print(f\"警告: 时间戳列中有 {duplicate_timestamps} 个重复值\")\n",
    "        if VERBOSE:\n",
    "            duplicate_indices = data[data['timestamp'].duplicated(keep='first')].index\n",
    "            print(f\"重复的时间戳索引: {duplicate_indices.tolist()}\")\n",
    "    \n",
    "    # 3. 检查时间间隔是否一致\n",
    "    # 先按时间戳排序\n",
    "    data = data.sort_values('timestamp')\n",
    "    \n",
    "    # 计算时间差\n",
    "    time_diff = data['timestamp'].diff()\n",
    "    \n",
    "    # 获取最常见的时间间隔(应该是1小时)\n",
    "    expected_interval = time_diff.mode().iloc[0]\n",
    "    print(f\"最常见的时间间隔: {expected_interval}\")\n",
    "    \n",
    "    # 找出不符合预期间隔的时间点(排除第一个点)\n",
    "    irregular_intervals = time_diff[1:][time_diff[1:] != expected_interval]\n",
    "    if not irregular_intervals.empty:\n",
    "        print(f\"发现 {len(irregular_intervals)} 个不规则的时间间隔\")\n",
    "        if VERBOSE:\n",
    "            for idx, interval in irregular_intervals.items():\n",
    "                current_time = data.loc[idx, 'timestamp']\n",
    "                previous_time = data.loc[idx-1, 'timestamp'] if idx > 0 else None\n",
    "                print(f\"索引 {idx}: {previous_time} -> {current_time}, 间隔: {interval}\")\n",
    "    \n",
    "    # 4. 检查是否覆盖了预期的完整时间范围\n",
    "    start_date = data['timestamp'].min()\n",
    "    end_date = data['timestamp'].max()\n",
    "    print(f\"数据时间范围: {start_date} 到 {end_date}\")\n",
    "    \n",
    "    # 创建理想的时间范围(每小时一个点)\n",
    "    ideal_range = pd.date_range(start=start_date, end=end_date, freq='h')\n",
    "    \n",
    "    # 检查是否有缺失的时间点\n",
    "    missing_times = set(ideal_range) - set(data['timestamp'])\n",
    "    if missing_times:\n",
    "        print(f\"发现 {len(missing_times)} 个缺失的时间点\")\n",
    "        if VERBOSE:\n",
    "            # 仅打印前10个缺失时间点(如果太多)\n",
    "            print_times = sorted(list(missing_times))[:10]\n",
    "            print(f\"部分缺失时间点: {print_times}\")\n",
    "            if len(missing_times) > 10:\n",
    "                print(f\"... 以及 {len(missing_times) - 10} 个其他缺失时间点\")\n",
    "    \n",
    "    # 5. 检查闰年2月29日是否存在\n",
    "    feb_29_date = pd.Timestamp('2016-02-29').date()\n",
    "    feb_29_data = data[data['timestamp'].dt.date == feb_29_date]\n",
    "    if len(feb_29_data) == 0:\n",
    "        print(\"警告: 数据中缺少2016年2月29日(闰年)的数据\")\n",
    "    elif len(feb_29_data) < 24:\n",
    "        print(f\"警告: 2016年2月29日数据不完整，只有{len(feb_29_data)}小时，应该有24小时\")\n",
    "    else:\n",
    "        print(f\"2016年2月29日(闰年)数据完整，共有{len(feb_29_data)}小时的数据\")\n",
    "    \n",
    "    # 6. 处理任何时间戳问题\n",
    "    # 处理重复的时间戳\n",
    "    if duplicate_timestamps > 0:\n",
    "        print(\"正在处理重复的时间戳...\")\n",
    "        # 对于重复的时间戳，保留第一个记录\n",
    "        data = data.drop_duplicates(subset='timestamp', keep='first')\n",
    "\n",
    "    # 处理缺失的时间点 - 创建带有标记的完整时间序列\n",
    "    if missing_times:\n",
    "        print(\"正在处理缺失的时间点...\")\n",
    "        # 创建一个完整的时间序列DataFrame\n",
    "        complete_index = pd.DataFrame(index=ideal_range)\n",
    "        # 将原始数据与完整索引合并\n",
    "        data = data.set_index('timestamp').join(complete_index, how='right')\n",
    "        # 添加一个标记列，标识哪些行是因缺失而插入的\n",
    "        data['is_imputed'] = data.isnull().all(axis=1)\n",
    "        # 重置索引，使timestamp成为列\n",
    "        data = data.reset_index().rename(columns={'index': 'timestamp'})\n",
    "    else:\n",
    "        # 如果没有缺失时间点，也添加标记列，全部标记为False\n",
    "        data['is_imputed'] = False\n",
    "    \n",
    "    # 删除site_id列（如果存在）\n",
    "    if 'site_id' in data.columns:\n",
    "        print(\"删除site_id列...\")\n",
    "        data = data.drop(columns=['site_id'])\n",
    "    \n",
    "    # 保存原始数据的副本，用于后续比较\n",
    "    original_data = data.copy()\n",
    "    \n",
    "    # 选择需要处理的数值列\n",
    "    numeric_cols = data.select_dtypes(include=['number']).columns.tolist()\n",
    "    # 从数值列中移除is_imputed标记列\n",
    "    if 'is_imputed' in numeric_cols:\n",
    "        numeric_cols.remove('is_imputed')\n",
    "    \n",
    "    print(f\"\\n处理以下列: {numeric_cols}\")\n",
    "    \n",
    "    # 查看缺失值情况\n",
    "    missing_stats = data[numeric_cols].isnull().sum()\n",
    "    print(\"\\n各列缺失值数量:\")\n",
    "    print(missing_stats)\n",
    "    \n",
    "    # 1. 改进的异常值检测 - 参考电力数据处理方法，使用更保守的方法\n",
    "    def detect_outliers_improved(df, columns, verbose=False):\n",
    "        \"\"\"使用更保守的方法检测异常值，区分原始0值和缺失值\"\"\"\n",
    "        outliers_dict = {}\n",
    "        \n",
    "        for col in columns:\n",
    "            # 首先只考虑非缺失的数据\n",
    "            valid_data = df[~df[col].isnull()]\n",
    "            \n",
    "            # 分别处理零值和非零值\n",
    "            non_zero_data = valid_data[valid_data[col] > 0]\n",
    "            \n",
    "            if len(non_zero_data) < 10:  # 如果非零数据太少，跳过此列\n",
    "                if verbose:\n",
    "                    print(f\"列'{col}'中非零值太少，跳过异常值检测\")\n",
    "                outliers_dict[col] = []\n",
    "                continue\n",
    "                \n",
    "            # 计算非零数据的统计量 - 使用更保守的百分位数\n",
    "            Q1 = non_zero_data[col].quantile(0.01)  # 更保守的下界\n",
    "            Q3 = non_zero_data[col].quantile(0.99)  # 更保守的上界\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            # 设置更宽松的界限，参考电力数据处理方法\n",
    "            lower_bound = max(Q1 - 5 * IQR, 0)  # 使用更大乘数，确保下界不小于0\n",
    "            upper_bound = Q3 + 5 * IQR\n",
    "            \n",
    "            # 对非零值进行异常检测\n",
    "            non_zero_outliers = valid_data[(valid_data[col] < lower_bound) & \n",
    "                                          (valid_data[col] > 0) | \n",
    "                                          (valid_data[col] > upper_bound)].index.tolist()\n",
    "            \n",
    "            # 对零值进行特殊处理\n",
    "            zero_outliers = []\n",
    "            zero_values = valid_data[valid_data[col] == 0]\n",
    "            \n",
    "            if len(zero_values) > 0:\n",
    "                # 检查每个零值是否可能是异常的\n",
    "                for idx in zero_values.index:\n",
    "                    # 获取时间特征\n",
    "                    current_timestamp = df.loc[idx, 'timestamp']\n",
    "                    current_hour = current_timestamp.hour\n",
    "                    current_month = current_timestamp.month\n",
    "                    current_dayofweek = current_timestamp.dayofweek\n",
    "                    \n",
    "                    # 查找相似时间点的数据\n",
    "                    similar_times_data = df[(df['timestamp'].dt.hour == current_hour) & \n",
    "                                        (df['timestamp'].dt.month == current_month) & \n",
    "                                        (~df[col].isnull()) &\n",
    "                                        (df[col] > 0)]\n",
    "                    \n",
    "                    # 如果相似时间点通常有非零值，则这个零可能是异常的\n",
    "                    if len(similar_times_data) >= 5 and similar_times_data[col].mean() > 0.5:\n",
    "                        zero_outliers.append(idx)\n",
    "            \n",
    "            # 合并非零异常值和异常零值\n",
    "            outliers_dict[col] = non_zero_outliers + zero_outliers\n",
    "            \n",
    "            print(f\"检测到{len(outliers_dict[col])}个异常值在'{col}'列 (包括{len(zero_outliers)}个可疑的零值)\")\n",
    "            if len(non_zero_outliers) > 0 and verbose:\n",
    "                print(f\"  非零值范围: 下界={lower_bound:.4f}, 上界={upper_bound:.4f}\")\n",
    "        \n",
    "        return outliers_dict\n",
    "    \n",
    "    # 2. 改进的异常值替换方法 - 参考电力数据处理，避免将非零值替换为零\n",
    "    def replace_outliers_improved(df, outliers_dict, verbose=False):\n",
    "        \"\"\"替换异常值，特别注意避免将非零值错误地替换为零\"\"\"\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # 确保所有数值列都是浮点型，避免类型不匹配问题\n",
    "        for col in numeric_cols:\n",
    "            df_clean[col] = df_clean[col].astype(float)\n",
    "        \n",
    "        total_replaced = 0\n",
    "        for col, idx_list in outliers_dict.items():\n",
    "            col_replaced = 0\n",
    "            if len(idx_list) > 0:\n",
    "                for idx in idx_list:\n",
    "                    # 获取当前异常值\n",
    "                    original_value = df.loc[idx, col]\n",
    "                    \n",
    "                    # 获取时间特征\n",
    "                    current_timestamp = df.loc[idx, 'timestamp']\n",
    "                    current_hour = current_timestamp.hour\n",
    "                    current_month = current_timestamp.month\n",
    "                    current_dayofweek = current_timestamp.dayofweek\n",
    "                    \n",
    "                    # 查找前后7天同一时间点的数据（参考电力数据处理方法）\n",
    "                    similar_values = []\n",
    "                    for days_offset in range(-7, 8):\n",
    "                        if days_offset == 0:\n",
    "                            continue  # 跳过当前日期\n",
    "                        \n",
    "                        offset_date = current_timestamp + pd.Timedelta(days=days_offset)\n",
    "                        similar_idx = df[(df['timestamp'].dt.hour == offset_date.hour) & \n",
    "                                        (df['timestamp'].dt.day == offset_date.day) & \n",
    "                                        (df['timestamp'].dt.month == offset_date.month)].index\n",
    "\n",
    "                        if len(similar_idx) > 0 and similar_idx[0] not in outliers_dict[col]:\n",
    "                            similar_value = df.loc[similar_idx[0], col]\n",
    "                            if not pd.isna(similar_value):\n",
    "                                similar_values.append(similar_value)\n",
    "                    \n",
    "                    # 如果找到足够的相似时间点数据\n",
    "                    if len(similar_values) >= 3:\n",
    "                        # 使用相似时间点的中位数替换异常值\n",
    "                        replacement_value = np.median(similar_values)\n",
    "                        \n",
    "                        # 重要：如果原始值不为零而替换值为零，保留原始值\n",
    "                        if original_value > 0 and replacement_value == 0:\n",
    "                            if verbose:\n",
    "                                print(f\"保留列'{col}'中索引{idx}的非零原始值({original_value})而不是替换为0\")\n",
    "                        else:\n",
    "                            df_clean.loc[idx, col] = replacement_value\n",
    "                            col_replaced += 1\n",
    "                            if verbose:\n",
    "                                print(f\"替换列'{col}'索引{idx}的异常值: {original_value} -> {replacement_value}\")\n",
    "                    else:\n",
    "                        # 如果没有足够的相似时间点，尝试使用同一小时段的数据\n",
    "                        same_hour_data = df[(df['timestamp'].dt.hour == current_hour) & \n",
    "                                          (~df[col].isnull()) & \n",
    "                                          (~df.index.isin(outliers_dict[col]))]\n",
    "                        \n",
    "                        if len(same_hour_data) >= 3:\n",
    "                            replacement_value = same_hour_data[col].median()\n",
    "                            \n",
    "                            # 同样检查：如果原始值不为零而替换值为零，保留原始值\n",
    "                            if original_value > 0 and replacement_value == 0:\n",
    "                                if verbose:\n",
    "                                    print(f\"保留列'{col}'中索引{idx}的非零原始值({original_value})而不是替换为0\")\n",
    "                            else:\n",
    "                                df_clean.loc[idx, col] = replacement_value\n",
    "                                col_replaced += 1\n",
    "                                if verbose:\n",
    "                                    print(f\"使用同时段数据替换列'{col}'索引{idx}的异常值: {original_value} -> {replacement_value}\")\n",
    "                        else:\n",
    "                            # 如果仍然没有足够数据，使用非零值的中位数\n",
    "                            non_zero_values = df[df[col] > 0][col].dropna()\n",
    "                            if len(non_zero_values) > 0:\n",
    "                                replacement_value = non_zero_values.median()\n",
    "                                \n",
    "                                # 再次检查是否会将非零值替换为零\n",
    "                                if original_value > 0 and replacement_value == 0:\n",
    "                                    if verbose:\n",
    "                                        print(f\"保留列'{col}'中索引{idx}的非零原始值({original_value})而不是替换为0\")\n",
    "                                else:\n",
    "                                    df_clean.loc[idx, col] = replacement_value\n",
    "                                    col_replaced += 1\n",
    "                                    if verbose:\n",
    "                                        print(f\"使用非零中位数替换列'{col}'索引{idx}的异常值: {original_value} -> {replacement_value}\")\n",
    "                            else:\n",
    "                                # 极端情况：保留原值\n",
    "                                if verbose:\n",
    "                                    print(f\"警告: 无法找到合适的替换值，保留列'{col}'索引{idx}的原始值: {original_value}\")\n",
    "                \n",
    "                print(f\"列'{col}'中替换了{col_replaced}个异常值\")\n",
    "                total_replaced += col_replaced\n",
    "        \n",
    "        print(f\"总共替换了{total_replaced}个异常值\")\n",
    "        return df_clean\n",
    "    \n",
    "    # 3. 改进的缺失值填充 - 参考电力数据处理，避免零值填充\n",
    "    def fill_missing_values_improved(df, columns, verbose=False):\n",
    "        \"\"\"智能填充缺失值，避免使用零值填充，保持数据特征和时间相关性\"\"\"\n",
    "        df_filled = df.copy()\n",
    "        total_filled = 0\n",
    "        \n",
    "        for col in columns:\n",
    "            # 检查是否有缺失值\n",
    "            missing_mask = df[col].isnull()\n",
    "            n_missing = missing_mask.sum()\n",
    "            \n",
    "            if n_missing > 0:\n",
    "                print(f\"'{col}'列中有{n_missing}个缺失值\")\n",
    "                col_filled = 0\n",
    "                \n",
    "                # 获取所有缺失行的信息\n",
    "                missing_indices = df[missing_mask].index\n",
    "                \n",
    "                for i, idx in enumerate(missing_indices):\n",
    "                    # 获取时间特征\n",
    "                    current_timestamp = df.loc[idx, 'timestamp']\n",
    "                    current_hour = current_timestamp.hour\n",
    "                    current_month = current_timestamp.month\n",
    "                    current_dayofweek = current_timestamp.dayofweek\n",
    "                    is_imputed = df.loc[idx, 'is_imputed']\n",
    "                    \n",
    "                    # 初始化similar_values列表\n",
    "                    similar_values = []\n",
    "                    \n",
    "                    # 1. 尝试使用前后7天同一时间点的数据（参考电力数据处理）\n",
    "                    for days_offset in range(-7, 8):\n",
    "                        if days_offset == 0:\n",
    "                            continue  # 跳过当前日期\n",
    "                        \n",
    "                        offset_date = current_timestamp + pd.Timedelta(days=days_offset)\n",
    "                        similar_idx = df[(df['timestamp'].dt.hour == offset_date.hour) & \n",
    "                                       (df['timestamp'].dt.day == offset_date.day) & \n",
    "                                       (df['timestamp'].dt.month == offset_date.month) &\n",
    "                                       (~df[col].isnull())].index\n",
    "                        \n",
    "                        if len(similar_idx) > 0:\n",
    "                            similar_value = df.loc[similar_idx[0], col]\n",
    "                            if not pd.isna(similar_value) and similar_value > 0:  # 只考虑非零值\n",
    "                                similar_values.append(similar_value)\n",
    "                    \n",
    "                    # 如果找到足够的相似时间点数据\n",
    "                    if len(similar_values) >= 3:\n",
    "                        # 使用相似时间点的中位数填充\n",
    "                        fill_value = np.median(similar_values)\n",
    "                        df_filled.loc[idx, col] = fill_value\n",
    "                        col_filled += 1\n",
    "                        if verbose and (i % 100 == 0 or i == n_missing - 1):\n",
    "                            print(f\"使用相似时间点的中位数填充列'{col}'索引{idx}的缺失值: {fill_value}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # 2. 如果找不到足够的相似时间点，尝试使用线性插值（对于插入的缺失时间点）\n",
    "                    if is_imputed:\n",
    "                        try:\n",
    "                            # 找出前后最近的非缺失值时间点\n",
    "                            prev_data = df[(df['timestamp'] < current_timestamp) & \n",
    "                                         (~df[col].isnull())].sort_values('timestamp', ascending=False)\n",
    "                            next_data = df[(df['timestamp'] > current_timestamp) & \n",
    "                                         (~df[col].isnull())].sort_values('timestamp')\n",
    "                            \n",
    "                            if not prev_data.empty and not next_data.empty:\n",
    "                                prev_idx = prev_data.index[0]\n",
    "                                next_idx = next_data.index[0]\n",
    "\n",
    "                                prev_value = df.loc[prev_idx, col]\n",
    "                                next_value = df.loc[next_idx, col]\n",
    "                                \n",
    "                                # 只有当两个值都不为零或者至少有一个非零值时才进行线性插值\n",
    "                                if prev_value > 0 or next_value > 0:\n",
    "                                    prev_time = df.loc[prev_idx, 'timestamp']\n",
    "                                    next_time = df.loc[next_idx, 'timestamp']\n",
    "                                    \n",
    "                                    # 计算时间差\n",
    "                                    time_diff_total = (next_time - prev_time).total_seconds()\n",
    "                                    time_diff_current = (current_timestamp - prev_time).total_seconds()\n",
    "                                    \n",
    "                                    # 线性插值\n",
    "                                    if time_diff_total > 0:\n",
    "                                        weight = time_diff_current / time_diff_total\n",
    "                                        interpolated_value = prev_value + (next_value - prev_value) * weight\n",
    "                                        \n",
    "                                        # 确保插值结果不为负\n",
    "                                        interpolated_value = max(0, interpolated_value)\n",
    "                                        \n",
    "                                        df_filled.loc[idx, col] = interpolated_value\n",
    "                                        col_filled += 1\n",
    "                                        if verbose and (i % 100 == 0 or i == n_missing - 1):\n",
    "                                            print(f\"使用线性插值填充列'{col}'索引{idx}的缺失值: {interpolated_value}\")\n",
    "                                        continue\n",
    "                        except Exception as e:\n",
    "                            if verbose:\n",
    "                                print(f\"线性插值过程中出错: {e}\")\n",
    "                    \n",
    "                    # 3. 如果前两种方法都失败，尝试使用相似时间模式的数据\n",
    "                    similar_times = df[\n",
    "                        (df['timestamp'].dt.month == current_month) & \n",
    "                        (df['timestamp'].dt.hour == current_hour) & \n",
    "                        ((df['timestamp'].dt.dayofweek >= 5) == (current_dayofweek >= 5)) &\n",
    "                        (~df[col].isnull()) &\n",
    "                        (df[col] > 0)  # 只考虑非零值\n",
    "                    ]\n",
    "                    \n",
    "                    if len(similar_times) >= 3:\n",
    "                        # 使用相似值的中位数填充缺失值\n",
    "                        fill_value = similar_times[col].median()\n",
    "                        df_filled.loc[idx, col] = fill_value\n",
    "                        col_filled += 1\n",
    "                        if verbose and (i % 100 == 0 or i == n_missing - 1):\n",
    "                            print(f\"使用相似时间模式的中位数填充列'{col}'索引{idx}的缺失值: {fill_value}\")\n",
    "                    else:\n",
    "                        # 4. 如果没有足够相似值，尝试放宽条件：只匹配小时\n",
    "                        broader_similar_times = df[\n",
    "                            (df['timestamp'].dt.hour == current_hour) &\n",
    "                            (~df[col].isnull()) &\n",
    "                            (df[col] > 0)  # 只考虑非零值\n",
    "                        ]\n",
    "                        \n",
    "                        if len(broader_similar_times) >= 3:\n",
    "                            fill_value = broader_similar_times[col].median()\n",
    "                            df_filled.loc[idx, col] = fill_value\n",
    "                            col_filled += 1\n",
    "                            if verbose and (i % 100 == 0 or i == n_missing - 1):\n",
    "                                print(f\"使用同时段数据的中位数填充列'{col}'索引{idx}的缺失值: {fill_value}\")\n",
    "                        else:\n",
    "                            # 5. 如果仍然没有足够数据，使用所有非零值的中位数\n",
    "                            non_zero_values = df[df[col] > 0][col].dropna()\n",
    "                            if len(non_zero_values) > 0:\n",
    "                                fill_value = non_zero_values.median()\n",
    "                                df_filled.loc[idx, col] = fill_value\n",
    "                                col_filled += 1\n",
    "                                if verbose and (i % 100 == 0 or i == n_missing - 1):\n",
    "                                    print(f\"使用所有非零值的中位数填充列'{col}'索引{idx}的缺失值: {fill_value}\")\n",
    "                            else:\n",
    "                                # 极端情况：使用一个小的非零值（而不是0）\n",
    "                                fill_value = 0.01  # 使用一个很小的非零值\n",
    "                                df_filled.loc[idx, col] = fill_value\n",
    "                                col_filled += 1\n",
    "                                if verbose:\n",
    "                                    print(f\"警告: 无法找到合适的填充值，使用小的非零值{fill_value}填充列'{col}'索引{idx}的缺失值\")\n",
    "                \n",
    "                print(f\"列'{col}'中填充了{col_filled}个缺失值\")\n",
    "                total_filled += col_filled\n",
    "        \n",
    "        print(f\"总共填充了{total_filled}个缺失值\")\n",
    "        return df_filled\n",
    "    \n",
    "    # 4. 最终检查函数 - 从电力数据处理借鉴，确保没有非零值被错误地替换为零\n",
    "    def ensure_nonzero_preserved(df_processed, df_original, columns):\n",
    "        \"\"\"确保所有列中的非零值不会被错误地设置为零\"\"\"\n",
    "        df_fixed = df_processed.copy()\n",
    "        total_fixed = 0\n",
    "        \n",
    "        for col in columns:\n",
    "            # 检查是否有被设为0的非零原始值\n",
    "            mask = (df_processed[col] == 0) & (df_original[col] > 0)\n",
    "            count = mask.sum()\n",
    "            \n",
    "            if count > 0:\n",
    "                print(f\"修复：发现{count}个'{col}'列中的非零值被错误地设置为0，正在恢复原值\")\n",
    "                # 恢复原始值\n",
    "                df_fixed.loc[mask, col] = df_original.loc[mask, col]\n",
    "                total_fixed += count\n",
    "        \n",
    "        if total_fixed > 0:\n",
    "            print(f\"总共修复了{total_fixed}个被错误设置为0的非零值\")\n",
    "        else:\n",
    "            print(\"没有发现被错误设置为0的非零值\")\n",
    "            \n",
    "        return df_fixed\n",
    "    \n",
    "    def normalize_all_numeric_data(df, columns):\n",
    "        \"\"\"对数据进行归一化处理，适当处理负值、零值和正值\"\"\"\n",
    "        print(\"\\n=== 执行数据归一化（全面处理） ===\")\n",
    "        df_normalized = df.copy()\n",
    "        \n",
    "        for col in columns:\n",
    "            try:\n",
    "                # 检查列的数据特性\n",
    "                valid_values = df[col].dropna()\n",
    "                min_value = valid_values.min()\n",
    "                max_value = valid_values.max()\n",
    "                zero_count = (valid_values == 0).sum()\n",
    "                has_negative = (valid_values < 0).any()\n",
    "                total_count = len(valid_values)\n",
    "                \n",
    "                print(f\"列 '{col}': 范围=[{min_value:.4f}, {max_value:.4f}], 零值数量={zero_count}, 含负值={has_negative}\")\n",
    "                \n",
    "                if total_count > 0:\n",
    "                    # 根据数据特性选择不同的归一化策略\n",
    "                    if col in ['airTemperature', 'dewTemperature'] or has_negative:\n",
    "                        # 对可能包含负值的温度数据使用标准MinMaxScaler\n",
    "                        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "                        values_2d = valid_values.values.reshape(-1, 1)\n",
    "                        scaler.fit(values_2d)\n",
    "                        \n",
    "                        # 对所有非缺失值进行归一化\n",
    "                        non_missing_mask = ~df[col].isnull()\n",
    "                        if any(non_missing_mask):\n",
    "                            values_to_transform = df.loc[non_missing_mask, col].values.reshape(-1, 1)\n",
    "                            normalized_values = scaler.transform(values_to_transform).flatten()\n",
    "                            df_normalized.loc[non_missing_mask, col] = normalized_values\n",
    "                        \n",
    "                        print(f\"归一化列 '{col}' (包含负值): 范围从 [{min_value:.4f}, {max_value:.4f}] 归一化到 [0, 1]\")\n",
    "                    else:\n",
    "                        # 对主要为正值的数据使用保留零值的方法\n",
    "                        non_zero_values = valid_values[valid_values != 0]\n",
    "                        if len(non_zero_values) > 0:\n",
    "                            scaler = MinMaxScaler(feature_range=(0.01, 1))\n",
    "                            non_zero_values_2d = non_zero_values.values.reshape(-1, 1)\n",
    "                            scaler.fit(non_zero_values_2d)\n",
    "                            \n",
    "                            # 对非零非缺失值进行归一化\n",
    "                            non_zero_mask = (df[col] != 0) & (~df[col].isnull())\n",
    "                            if any(non_zero_mask):\n",
    "                                values_to_transform = df.loc[non_zero_mask, col].values.reshape(-1, 1)\n",
    "                                normalized_values = scaler.transform(values_to_transform).flatten()\n",
    "                                \n",
    "                                # 更新归一化后的值\n",
    "                                normalized_col = df[col].copy()\n",
    "                                normalized_col.loc[non_zero_mask] = normalized_values\n",
    "                                df_normalized[col] = normalized_col\n",
    "                            \n",
    "                            print(f\"归一化列 '{col}' (保留零值): 非零值范围从 [{non_zero_values.min():.4f}, {non_zero_values.max():.4f}] 归一化到 [0.01, 1]\")\n",
    "                        else:\n",
    "                            print(f\"警告: 列 '{col}' 没有非零值，跳过归一化\")\n",
    "                else:\n",
    "                    print(f\"警告: 列 '{col}' 没有有效值，跳过归一化\")\n",
    "            except Exception as e:\n",
    "                print(f\"归一化列 '{col}' 时出错: {str(e)}\")\n",
    "                # 保持原始值\n",
    "        \n",
    "        return df_normalized\n",
    "    \n",
    "    # 执行数据处理流程\n",
    "    print(\"\\n=== 开始数据处理 ===\")\n",
    "    \n",
    "    # 1. 检测异常值\n",
    "    print(\"\\n检测异常值...\")\n",
    "    outliers = detect_outliers_improved(data, numeric_cols, verbose=VERBOSE)\n",
    "    \n",
    "    # 2. 替换异常值\n",
    "    print(\"\\n替换异常值...\")\n",
    "    data_cleaned = replace_outliers_improved(data, outliers, verbose=VERBOSE)\n",
    "    \n",
    "    # 3. 填充缺失值\n",
    "    print(\"\\n填充缺失值...\")\n",
    "    data_filled = fill_missing_values_improved(data_cleaned, numeric_cols, verbose=VERBOSE)\n",
    "    \n",
    "    # 4. 最终检查 - 确保非零值不被错误地替换为零\n",
    "    print(\"\\n执行最终检查...\")\n",
    "    data_checked = ensure_nonzero_preserved(data_filled, original_data, numeric_cols)\n",
    "    \n",
    "    # 5. 数据归一化 - 使用全面处理的方法\n",
    "    print(\"\\n执行数据归一化...\")\n",
    "    data_normalized = normalize_all_numeric_data(data_checked, numeric_cols)\n",
    "    \n",
    "    # 删除标记列（处理完成后不再需要）\n",
    "    if 'is_imputed' in data_normalized.columns:\n",
    "        data_normalized = data_normalized.drop(columns=['is_imputed'])\n",
    "    \n",
    "    # 检查处理后的数据质量\n",
    "    print(\"\\n=== 数据处理完成 ===\")\n",
    "    print(f\"原始数据中缺失值总数: {original_data[numeric_cols].isnull().sum().sum()}\")\n",
    "    print(f\"处理后数据中缺失值总数: {data_normalized[numeric_cols].isnull().sum().sum()}\")\n",
    "    \n",
    "    # 计算原始数据和处理后数据的0值数量\n",
    "    original_zeros = sum((original_data[numeric_cols] == 0).sum())\n",
    "    processed_zeros = sum((data_normalized[numeric_cols] == 0).sum())\n",
    "    print(f\"原始数据中0值总数: {original_zeros}\")\n",
    "    print(f\"处理后数据中0值总数: {processed_zeros}\")\n",
    "    \n",
    "    # 验证处理结果\n",
    "    def validate_processing(original, processed, columns):\n",
    "        \"\"\"验证处理结果，确保没有非零值被错误地设置为零\"\"\"\n",
    "        all_valid = True\n",
    "        \n",
    "        for col in columns:\n",
    "            # 检查是否有非零值被设为零\n",
    "            mask = (processed[col] == 0) & (original[col] > 0) & (~original[col].isnull())\n",
    "            zero_count = mask.sum()\n",
    "            \n",
    "            if zero_count > 0:\n",
    "                print(f\"警告: 列'{col}'中有{zero_count}个非零值被错误地设置为0\")\n",
    "                all_valid = False\n",
    "                \n",
    "                if VERBOSE:\n",
    "                    # 显示部分被错误修改的值\n",
    "                    problem_indices = original[mask].index[:5]  # 最多显示5个\n",
    "                    for idx in problem_indices:\n",
    "                        print(f\"  索引 {idx}: 原值 {original.loc[idx, col]} -> 新值 0\")\n",
    "            \n",
    "            # 检查是否有缺失值\n",
    "            missing_count = processed[col].isnull().sum()\n",
    "            if missing_count > 0:\n",
    "                print(f\"警告: 列'{col}'中仍有{missing_count}个缺失值\")\n",
    "                all_valid = False\n",
    "        \n",
    "        if all_valid:\n",
    "            print(\"验证通过: 所有非零值都被正确保留，没有缺失值\")\n",
    "        \n",
    "        return all_valid\n",
    "    \n",
    "    # 执行验证\n",
    "    print(\"\\n=== 验证处理结果 ===\")\n",
    "    validation_result = validate_processing(original_data, data_normalized, numeric_cols)\n",
    "    \n",
    "    # 5. 为不同的数据集划分并保存数据\n",
    "    print(\"\\n=== 划分并保存数据集 ===\")\n",
    "    for dataset_name, (start_time, end_time) in dataset_types.items():\n",
    "        # 根据时间范围筛选数据\n",
    "        mask = (data_normalized['timestamp'] >= start_time) & (data_normalized['timestamp'] <= end_time)\n",
    "        subset = data_normalized[mask].copy()\n",
    "        \n",
    "        if len(subset) == 0:\n",
    "            print(f\"警告: {dataset_name}数据集为空，跳过保存\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"划分{dataset_name}数据集，包含{len(subset)}行数据\")\n",
    "        \n",
    "        # 保存处理后的数据\n",
    "        output_file = f\"{output_dir}/{file_base}_{dataset_name}.csv\"\n",
    "        subset.to_csv(output_file, index=False)\n",
    "        print(f\"已保存到{output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e088f4c3-30b3-4774-975b-d1f005c5b55d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
